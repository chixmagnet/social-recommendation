Having surveyed previous CF work, especially MF-based CF and SCF
methods, we now present the major conceptual contributions of the
paper.  We begin by introducing a unified
matrix factorization framework for optimizing all MF objectives
evaluated in this paper --- old and new.
%, including newly proposed objectives to
%address observed deficiencies of SCF MF methods as outlined in
%Section~\ref{sec:Introduction}.

\subsection{A Composable Objective Framework}

%\subsubsection{Composable Objectives}

We take a composable approach to MF-based (S)CF, where an optimization
objective $\mathit{Obj}$ is composed of sums of one or more objective
components:
\begin{align}
\mathit{Obj} = \sum_i \lambda_i \mathit{Obj}_i
\end{align}
Because each objective may be weighted differently, a weighting term
$\lambda_i \in \R$ is included for each component.  In the current
work, we manually tuned each $\lambda_i$, except for the \emph{first} 
$i$ in $\sum_i$, which can be set as $\lambda_i = 1$ without loss of
generality.

Most target predictions in this paper are binary ($\{0,1\}$),
therefore a sigmoidal transform $\sigma(o) = \frac{1}{1 + e^{-o}}$ of
a prediction $o \in \R$ may be used to squash it to the range $[0,
1]$.  Where the $\sigma$ transform may be optionally
included, this is written as $[\sigma]$.  While $\sigma$ transforms
are generally advocated for real-valued regressor outputs when used in
a classification setting, we note that our experiments showed little
variation in results whether including or omitting it, although
including it tended to slow the convergence of gradient optimization.
Nonetheless, where appropriate, we include the possibility of a
$\sigma$ transform since it may prove useful in other settings.

\subsection{Existing Objective Functions}

For completeness, we first cover a number of known objective
components that are used in the objectives evaluated and
extended in this paper.  A discussion of gradient optimization along
with all necessary derivatives for these objectives is provided in
Appendix~\ref{app:Derivatives}.

\subsubsection{Matchbox-style Matrix Factorization ($\Obj_\pmcf$)}

\label{sec:matchbox_def}

%As a first step towards addressing our first observed deficiency that
%%the six surveyed SCF MF methods of Section~\ref{sec:scf_original} do not
%exploit user or item features.  In extending these methods to support
%features, we must first identify
In Section~\ref{sec:mf}, we discussed an MF objective~\eqref{eq:basic_mf}
that \emph{did not} make use of user or item features.  
However, if we \emph{do} have user feature vector $\x$ 
and item feature vector $\y$, we can respectively
represent the latent projections of user and item as $(U \x)_{1 \ldots
K}$ and $(V \y)_{1 \ldots K}$ and hence use $\la U \x, V \y \ra = \x^T
U^T V \y$ as a measure of affinity between user $\x$ and item $\y$.
Substituting the feature-based $\x^T U^T V \y$ for the featureless 
$U_\x^T V_\y$ in~\eqref{eq:basic_mf}, we arrive at the form of the basic CF 
objective function used in \emph{Matchbox}~\cite{matchbox} --- although
Matchbox used Bayesian optimization methods, we can easily express
its main objective in the following log likelihood form:
\begin{align}
\Obj_\pmcf & = \sum_{(\x,\y) \in D} \frac{1}{2} (R_{\x,\y} - [\sigma] \x^T U^T V \y)^2 \label{eq:matchbox_obj}
\end{align}
%It should be clear that if $\x$ and $\y$ are simply binary vectors
%of mutually exclusive user and item IDs, then this form reduces
%to~\eqref{eq:basic_mf}.  However, in general it permits general real-valued
%feature vectors for $\x$ and $\y$ and hence a feature-based MF approach to CF.

\subsubsection{Regularization of $U$, $V$ \& $\w$ ($\Obj_\ru, \Obj_\rv, \Obj_\w$)}
%
To help in generalization, it is important to regularize any free
matrix parameters $U$ and $V$ (e.g., from
Section~\ref{sec:matchbox_def}) or vector parameters $\w$ (e.g., from
Section~\ref{sec:cbf}) to prevent overfitting  
% in the presence of
when dealing with 
sparse data. This can be done with a simple $L_2$ regularizer that
models a spherical Gaussian prior % of $0$  % ebonilla
on the parameters.  This
regularization component can be specified for $U$, $V$, and $\w$ 
% simply % ebonilla
 as follows:
\begin{align}
\Obj_\ru & = \frac{1}{2} \| U \|_\Fro^2 = \frac{1}{2} \tr(U^T U) \qquad
\Obj_\rv = \frac{1}{2} \tr(V^T V) \nonumber \\
\Obj_\rw & = \frac{1}{2} \| \w \|_2^2 = \frac{1}{2} \w^T \w \label{eq:reg}
\end{align}

\subsection{New Objective Functions}

\label{sec:newobjfun_defs}

Now we return to our observations concerning the deficiencies of
existing SCF MF methods as outlined in Section~\ref{sec:Introduction}
and propose new objective functions to address these issues.
Gradient-based optimization for these new objectives and all necessary
derivatives are covered in Appendix~\ref{app:Derivatives}.

\subsubsection{Feature Social Regularization ($\Obj_\rs$ \& $\Obj_\rss$)}
\label{sec:SocRec}

Our previous discussion of SCF methods in Section~\ref{sec:scf_original}
covered three different methods for \emph{social regularization} --- that is,
constraining users to be similar based on evidence from the social network.
However, none of these previous three SCF social regularization methods
exploited user features in the \emph{learning} process; more precisely 
$U_\x$ and $U_\z$ were regularized, but not the feature-based latent
spaces $U\x$ and $U\z$.  Hence if a gender difference in $\x$ and $\z$ was the
crucial factor to differentiating the latent spaces of each user, this could 
be \emph{learned} if we had a way of socially regularizing $U\x$ and $U\z$
directly.  To address this, we provide two variants of 
\emph{feature-based social regularization}.

The first new objective is an extension of simple 
\emph{social regularization}~\cite{lla,socinf} by incorporating user
features into that objective:
\begin{align}
\Obj_\rs = & \sum_{\x} \sum_{\z \in \mathit{friends}(\x)} \frac{1}{2} (S_{\x,\z} - \la U\x, U\z \ra)^2 \\
& = \sum_{\x} \sum_{\z \in \mathit{friends}(\x)} \frac{1}{2} (S_{\x,\z} - \x^T U^T U \z)^2 \nonumber 
\end{align}

%\subsubsection{Social Spectral Regularization ($\Obj_\rss$)}

Alternately, we could extend \emph{social spectral
regularization}~\cite{sr,rrmf} by incorporating user features into its
objective:
\begin{align}
\Obj_\rss = & \sum_{\x} \sum_{\z \in \mathit{friends}(\x)} \frac{1}{2} S^+_{\x,\z} \| U\x - U\z \|_2^2 \\
%& = \sum_{\x} \sum_{\z \in \mathit{friends}(\x)} \frac{1}{2} S^+_{\x,\z} \| U (\x - \z) \|_2^2 \nonumber \\
& = \sum_{\x} \sum_{\z \in \mathit{friends}(\x)} \frac{1}{2} S^+_{\x,\z} (\x - \z)^T U^T U (\x - \z) \nonumber
\end{align}

While we note these extensions are straightforward, they have the
simple property that they allow the system to learn the latent user
projection matrix $U$ \emph{as a function of user features} in order
to minimize the social regularization penalty.  Just as the Matchbox
objective in Section~\ref{sec:matchbox_def} allows us to exploit user
and item features in MF-based CF, these new social regularization
objectives permit more flexibility in exploiting user features in
\emph{learning} user similarity.
%Compared to non-socially regularized CF objectives, we will
%demonstrate that both of these approaches are quite powerful in 
%Section~\ref{sec:EmpResults}.

%\subfive Note: standard spectral regularization 
%assumes $S^+_{\x,\z} \in [0,1]$;
%however we may also want to try $S_{\x,\z}$ since a negative value
%actively encourages the latent spaces to oppose each other, which may
%be desired.

\subsubsection{Hybrid Information Diffusion + SCF ($\Obj_\phy$ )}

\label{sec:hybrid_scf}

One major weakness of MF methods is that they cannot model direct
joint features over user and items --- they must model user and item
features independently in order to compute the independent latent
projections $U\x$ and $U\z$.  Unfortunately, this prevents standard MF
objectives from modeling direct user-to-user information
diffusion~\cite{inf_diffusion} --- the unidirectional flow of
information (e.g., links) from one user to another.  This is
problematic because if user $\x$ \emph{always} likes what $\z$ has
posted or liked, then we would like to shortcut the latent
representation and simply learn to recommend user $\z$'s liked or 
posted items to user $\x$.

We fix this deficiency of MF by introducing another objective component
in addition to the standard MF objective, which serves as
a simple linear regressor for such information diffusion
observations.  The resulting hybrid objective component then becomes a
combination of latent MF and linear regression objectives.

For the linear regressor $\w^T \f_{\x,\y}$, we make use of the
\emph{same} weight vector $\w$ and feature vector $\f_{\x,\y}$
mentioned in Section~\ref{sec:cbf}; $\f_{\x,\y}$ is fully defined for
our empirical evaluation in Section~\ref{sec:fxy_def}.  As previously
noted, $\f_{\x,\y}$ includes
\emph{joint} user and item features from the social network, in
particular binary
\emph{information diffusion}~\cite{inf_diffusion} features
for \emph{each} friend $\z \in \mathit{friends}_\x$ indicating if $\z$
liked (or disliked) $\y$.  As a consequence, learning $\w$ allows the
linear regressor to predict in a personalized way for any user $\x$
whether they are likely to follow their friend $\z$'s preference for $\y$.

Formally, to define our hybrid information diffusion plus SCF objective,
we additively combine the output of the linear regression prediction with the
Matchbox prediction:
\begin{align}
\sqm \Obj_\phy = \sum_{(\x,\y) \in D} \frac{1}{2} (R_{\x,\y} - [\sigma] \w^T \f_{\x,\y} - [\sigma] \x^T U^T V y)^2 
\end{align}
%While again we note that this simple hybrid MF plus linear regression
%objective is straightforward, the ability to use joint user and item
%features to model information diffusion between users turns out to be 
%extremely powerful in our later experiments.

\subsubsection{Co-preference Regularization ($\Obj_\rsc$)}
\label{sec:rsc}

A crucial aspect missing from other SCF methods is that while two
users may not be globally similar or opposite in their preferences,
there may be sub-areas of their interests which can be correlated to
each other.  For example, two friends may have similar interests
concerning technology news, but different interests concerning
political news.  \emph{Co-preference regularization} aims to learn
such selective co-preferences.  The motivation is to constrain users
$\x$ and $\z$ who have similar or opposing preferences to be similar
or opposite in the \emph{same} latent item space relevant to item $\y$.

We use $\la \cdot, \cdot \ra_{\bullet}$ to denote a re-weighted inner
product. The purpose of this inner product is to ``mask'' enforcement
of latent space similarities or dissimilarities between users to be restricted
to the same latent spaces as the co-preferred items.  To this end, the
objective component for \emph{co-preference regularization} 
along with its expanded form is
\begin{align}
\Obj_\rsc & = \sum_{(\x,\z,\y) \in C} \frac{1}{2} (P_{\x,\z,\y} - \la U\x, U\z \ra_{V\y})^2 \\
& = \sum_{(\x,\z,\y) \in C} \frac{1}{2} (P_{\x,\z,\y} - \x^T U^T \diag(V\y) U \z)^2 \nonumber
\end{align}
We might also define a \emph{social co-preference spectral
regularization} approach, but our experiments so far have not indicated
this works as well as the above objective.  

In contrast to social regularization
defined previously, co-preference regularization does not require knowledge
of friendships or user interactions to determine co-preferences and hence
can enforce regularization constraints between \emph{all} users.
%can 
%can find correlations between users who are not friends --- this important
%observation will indeed surface in our forthcoming experimental evaluation.

\begin{comment}
\subsubsection{Social Co-preference Spectral Regularization ($\Obj_\rscs$)}
This is the same as the social co-preference regularization above, except that it uses the spectral regularizer format for 
learning the co-preferences.

 We use $\| \cdot \|_{2,\bullet}$ to denote a re-weighted $L_2$ norm. The reweighing of this norm servers the same purpose as the re-weighted inner product in Section~\ref{sec:rsc}, it tailors the similarities or dissimilarities between users to specific sets of items. This allows users $\x$
and $\z$ to be similar or opposite in the same latent latent space
relevant only to item $\y$.  
 
 The objective component for
 social co-preference spectral regularization along with its expanded form is
 
\begin{align}
\Obj_\rscs & = \sum_{(\x,\z,\y) \in C} \frac{1}{2} P_{\x,\z,\y} \| U\x - U\z \|_{2,V\y}^2 \\
%& = \sum_{(\x,\z,\y) \in C} \frac{1}{2} P_{\x,\z,\y} \| U (\x - \z) \|_{2,V\y}^2 \nonumber \\
& = \sum_{(\x,\z,\y) \in C} \frac{1}{2} P_{\x,\z,\y} (\x - \z)^T U^T \diag(V\y) U (\x - \z) \nonumber 
\end{align}
\end{comment}
