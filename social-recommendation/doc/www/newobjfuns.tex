In this section we introduce a unified matrix factorization framework
for expressing all MF objectives evaluated in this paper, including
newly proposed objectives to address observed deficiencies of SCF
MF methods as outlined in Section~\ref{sec:Introduction}

\subsection{Mathematical Preliminaries}

\subsubsection{Composable Objectives}

We take a composable approach to (S)CF where an optimization objective
$\mathit{Obj}$ is composed of sums of one or more objective
components:
\begin{align}
\mathit{Obj} = \sum_i \lambda_i \mathit{Obj}_i
\end{align}
Because each objective may be weighted differently, a weighting term
$\lambda_i \in \R$ is included for each component.

Most target predictions are binary classification-based ($\{0,1\}$),
therefore a sigmoidal transform $\sigma(o) = \frac{1}{1 + e^{-o}}$ of
a prediction $o \in \R$ may be used to squash it to the range $[0,
1]$.  In places where the $\sigma$ transform may be optionally
included, this is written as $[\sigma]$.  While $\sigma$ transforms 
are generally advocated for real-valued regressor outputs when used in
a classification setting, we note that our experiments
showed little variation in results whether including or omitting it.
Nonetheless we include the possibility of a $\sigma$ transform 
here where appropriate since it may prove useful in other settings.

\subsection{Existing Objective Functions}

\subsubsection{Matchbox Matrix Factorization ($\Obj_\pmcf $)}

%As a first step towards addressing our first observed deficiency that
%%the six surveyed SCF MF methods of Section~\ref{sec:scf_original} do not
%exploit user or item features.  In extending these methods to support
%features, we must first identify

If we \emph{do} have user and item features, we can respectively
represent the latent projections of user and item as $(U \x)_{1 \ldots
K}$ and $(V \y)_{1 \ldots K}$ and hence use $\la U \x, V \y \ra = \x^T
U^T V \y$ as a measure of affinity between user $\x$ and item $\y$.
Now we are faced with what objective to optimize in this feature-based
MF case; fortunately, the answer comes in the form of the basic
objective function used in Matchbox~\cite{matchbox} --- although
Matchbox used Bayesian optimization methods, we can easily express
its objective in the following log likelihood form:
\begin{align}
\Obj_\pmcf & = \sum_{(\x,\y) \in D} \frac{1}{2} (R_{\x,\y} - [\sigma] \x^T U^T V \y)^2
\end{align}

\subsubsection{$L_2$ Regularization of $U$ and $V$ ($\Obj_\ru, \Obj_\rv$)}

To help in generalization, it is important to regularize the free
parameters $U$ and $V$ to prevent overfitting in the presence of
sparse data. This can be done with the $L_2$ regularizer that models a
prior of $0$ on the parameters. The regularization component for $U$
is
\begin{align}
\Obj_\ru & = \frac{1}{2} \| U \|_\Fro^2 = \frac{1}{2} \tr(U^T U)
\end{align}
We can state the identical case substituting $V$ for $U$.

In the same manner as $U$ and $V$, it is important to regularize the
free parameter $\w$ to prevent overfitting in the presence of sparse
data.  This can again be done with the $L_2$ regularizer that models a
prior of $0$ on the parameters. The objective and gradient for the
$L_2$ regularizer for $\w$ is:
\begin{align}
 \Obj_\rw & = \frac{1}{2} \| \w \|_2^2 = \frac{1}{2} \w^T \w\\
\end{align}

\subsection{New Objective Functions}

\subsubsection{Social Regularization ($\Obj_\rs$)}
\label{sec:SocRec}

The social aspect of SCF is implemented as a regularizer on the user
matrix $U$. What this objective component does is constrain users with
a high similarity rating to have the same values in the latent feature
space. This models the assumption that users who are similar socially
should also have similar preferences for items.

This method is an extension of existing SCF
techniques~\cite{lla,socinf} described in Section~\ref{sec:mf} that
constrain the latent space to enforce users to have similar
preferences latent representations when they interact heavily.  Like
Matchbox which extends regular matrix factorization methods by making
use of user and link features, our extension to the Social
Regularization method incorporates user features to learn similarities
between users in the latent space.
\begin{align}
\Obj_\rs = & \sum_{\x} \sum_{\z \in \mathit{friends}(\x)} \frac{1}{2} (S_{\x,\z} - \la U\x, U\z \ra)^2 \\
& = \sum_{\x} \sum_{\z \in \mathit{friends}(\x)} \frac{1}{2} (S_{\x,\z} - \x^T U^T U \z)^2 \nonumber 
\end{align}

\subsubsection{Social Spectral Regularization ($\Obj_\rss$)}

As we did with the Social Regularization method in
Section~\ref{sec:SocRec}, we build on ideas used in
Matchbox~\cite{matchbox} to extend social spectral
regularization~\cite{sr,rrmf} by incorporating user features into the
objective.  The objective function for our extension to social spectral
regularization is:
\begin{align}
\Obj_\rss = & \sum_{\x} \sum_{\z \in \mathit{friends}(\x)} \frac{1}{2} S^+_{\x,\z} \| U\x - U\z \|_2^2 \\
%& = \sum_{\x} \sum_{\z \in \mathit{friends}(\x)} \frac{1}{2} S^+_{\x,\z} \| U (\x - \z) \|_2^2 \nonumber \\
& = \sum_{\x} \sum_{\z \in \mathit{friends}(\x)} \frac{1}{2} S^+_{\x,\z} (\x - \z)^T U^T U (\x - \z) \nonumber
\end{align}

%\subfive Note: standard spectral regularization 
%assumes $S^+_{\x,\z} \in [0,1]$;
%however we may also want to try $S_{\x,\z}$ since a negative value
%actively encourages the latent spaces to oppose each other, which may
%be desired.

\subsubsection{Hybrid Objective ($\Obj_\phy$ )}

As specified in Chapter 1, one weakness of MF methods is that they
cannot model joint features over user and items, and hence the cannot
model direct user-user information diffusion. Information diffusion
models the unidirectional flow of links from one user to another
(i.e., one user likes/shares what another user posts). We believe that
this information will be useful for SCF, and is lacking in current SCF
methods.

We fix this by introducing another objective component in addition to
the standard MF objective, and this component serves as a simple
linear regressor for such information diffusion observations. The
resulting hybrid objective component becomes a combination of latent
MF and linear regression objectives.

We make use of the $\f_{\x,\y}$ features detailed in
Section~\ref{sec:dataset} to make the linear regressor. $\f_{\x,\y}$
models user-user information diffusion because it is a joint feature
between users and the links. It allows us to learn information
diffusion models whether certain users always likes content from
another specific user.

%the $F'_{\x,\y}$ indicator function of which of the user's friends have liked a particular link along with the user.

Using $\la \cdot,\cdot \ra$ to denote an inner product, we define a
weight vector $\w \in \R^F$ such that $\la \w, \f_{\x,\y} \ra = \w^T
\f_{\x,\y}$ is the prediction of the system.  The objective of the
linear regression component is therefore
\begin{align*}
\Obj_\phy = \sum_{(\x,\y) \in D} \frac{1}{2} (R_{\x,\y} - [\sigma] \w^T \f_{\x,\y})^2
\end{align*}

We combine the output of the linear regression objective with the
Matchbox output $[\sigma] \x^T U^T V y$, to get a hybrid objective
component. The full objective function for this hybrid model is
\begin{align}
\sum_{(\x,\y) \in D} \frac{1}{2} (R_{\x,\y} - [\sigma] \w^T \f_{\x,\y} - [\sigma] \x^T U^T V y)^2
\end{align}

\subsubsection{Social Co-preference Regularization ($\Obj_\rsc$)}
\label{sec:rsc}

A crucial aspect missing from other SCF methods is that while two
users may not be globally similar or opposite in their preferences,
there may be sub-areas of their interests which can be correlated to
each other.  For example, two friends may have similar interests
concerning music, but different interests concerning politics.  The
social co-preference regularizers aim to learn such selective
co-preferences. The motivation is to constrain users $\x$ and $\z$ who
have similar or opposing preferences to be similar or opposite in the
same latent latent space relevant to item $\y$.

We use $\la \cdot, \cdot \ra_{\bullet}$ to denote a re-weighted inner
product. The purpose of this inner product is to tailor the latent
space similarities or dissimilarities between users to specific sets
of items. This fixes the issue detailed in the previous paragraph by
allowing users $\x$ and $\z$ to be similar or opposite in the same
latent latent space relevant only to item $\y$.

The objective component for social co-preference regularization along
with its expanded form is
\begin{align}
\Obj_\rsc & = \sum_{(\x,\z,\y) \in C} \frac{1}{2} (P_{\x,\z,\y} - \la U\x, U\z \ra_{V\y})^2 \\
& = \sum_{(\x,\z,\y) \in C} \frac{1}{2} (P_{\x,\z,\y} - \x^T U^T \diag(V\y) U \z)^2 \nonumber
\end{align}
We might also define a co-preference spectral regularization approach,
but our experiments have indicated this does not work as well as the
non-spectral objective above, so we omit it here.

\begin{comment}
\subsubsection{Social Co-preference Spectral Regularization ($\Obj_\rscs$)}
This is the same as the social co-preference regularization above, except that it uses the spectral regularizer format for 
learning the co-preferences.

 We use $\| \cdot \|_{2,\bullet}$ to denote a re-weighted $L_2$ norm. The reweighing of this norm servers the same purpose as the re-weighted inner product in Section~\ref{sec:rsc}, it tailors the similarities or dissimilarities between users to specific sets of items. This allows users $\x$
and $\z$ to be similar or opposite in the same latent latent space
relevant only to item $\y$.  
 
 The objective component for
 social co-preference spectral regularization along with its expanded form is
 
\begin{align}
\Obj_\rscs & = \sum_{(\x,\z,\y) \in C} \frac{1}{2} P_{\x,\z,\y} \| U\x - U\z \|_{2,V\y}^2 \\
%& = \sum_{(\x,\z,\y) \in C} \frac{1}{2} P_{\x,\z,\y} \| U (\x - \z) \|_{2,V\y}^2 \nonumber \\
& = \sum_{(\x,\z,\y) \in C} \frac{1}{2} P_{\x,\z,\y} (\x - \z)^T U^T \diag(V\y) U (\x - \z) \nonumber 
\end{align}
\end{comment}