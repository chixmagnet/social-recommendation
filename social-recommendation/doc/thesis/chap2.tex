%%
% Template chap2.tex
%%

\chapter{Background}

\section{Notation}

The following are mathematical notations common to the SCF setting and models explored in this paper:

\begin{itemize}
\item $N$ users, each having an $I$-element feature vector 
$\x \in \R^I$ (alternately if a second user is needed, $\z \in \R^I$).

\item $M$ items, each having a $J$-element feature vector 
$\y \in \R^J$. The feature vectors for users 
and items can consist of any real-valued features as well as $\{0,1\}$
features like user and item IDs.
%$\x_{1 \ldots I}$ ($\z_{1 \ldots I}$).  
%$\y_{1 \ldots J}$.s

\item A (non-exhaustive) data set $D$ of user preferences of the form
$D = \{ (\x, \y) \to R_{\x,\y} \}$ where class 
$R_{\x,\y} \in \{ 0 \; \mbox{(dislike)}, 1 \; \mbox{(like)} \}$.

\item A (non-exhaustive) data set $C$ of co-preferences derived from $D$ of the form
$C = \{ (\x, \z, \y) \to P_{\x, \z, \y} \}$ where class 
$P_{\x, \z, \y} \in \{ -1 \; \mbox{(disagree)}, 1 \;\mbox{(agree)} \}$.  

\item A similarity rating $S_{\x,\z}$ between any users $\x$ and $\z$. This is used to summarize all social
interaction between user $\x$ and user $\z$ in the term $S_{\x,\z} \in
\R$.  A definition of $S_{\x,\z} \in \R$ that has been useful is the
following:
\begin{align}
\mathit{Int}_{\x,\z} & = \frac{\mbox{\# interactions between $\x$
and $\z$}}{\mbox{average \# interactions between all user pairs}}\\
S_{\x,\z} & = \ln \left( \mathit{Int}_{\x,\z} \right)
\end{align}
For purposes of this definition, an \emph{interaction} is any single event
showing evidence that users $\x$ and $\z$ have 
interacted, e.g., a message exchange or being tagged in a photo together.

In addition, we can define $S^+_{\x,\z}$, a \emph{non-negative} 
variant of $S_{\x,\z}$:
\begin{align}
S^+_{\x,\z} & = \ln \left( 1 + \mathit{Int}_{\x,\z} \right)
\end{align}
\end{itemize}

The matrix $R$ is a sparse $N \times M$ matrix of user ratings on items. The problem of recommendation is filling out the empty elements of this matrix, and this can be looked at as a linear regression problem. There are two general ways that this has been done previously, Content-based Filtering (CBF) and Collaborative Filtering (CF). Content-based filtering makes recommendations based on correlations between the item features and the user's preferences on other items. In collaborative filtering, the system makes recommendations based on the correlation between other user's with similar preferences. Most traditional CBF methods learn in an explicit feature space, while most traditional CF methods learn in a latent feature space. 

\section{Related work}

There is a massive amount of related work on 
SCF~\cite{matchbox,ste,lla,glfm,tf,sorec,sr,rrmf,bisim,socinf} embodying some of the
ideas above, however there are a few aspects covered here, not covered
in this related work:
\begin{enumerate}
\item Existing SCF methods \emph{cannot} capture some of the basic features that are used in standard CBF systems due to the inherent independent factorization between user and items (e.g., how much one user follows another) --- this is the motivation behind the \emph{hybrid} objectives.
\item All methods \emph{except} for Matchbox~\cite{matchbox} ignore the issue of user and item features.  We extend the Matchbox approach above in our SCF methods. 
\item \emph{None} of the methods that propose social regularization~\cite{ste,sr,rrmf,lla,glfm,socinf} incorporate user features into this regularization (as done above).
\item Tensor-based factorizations such as~\cite{tf} use a full $K \times K \times K$ tensor for collaborative filtering w.r.t.\ tag prediction for users and items.  While our co-preference regularization models above were motivated by tensor approaches, we instead take an item-reweighted approach to the standard inner products to (a) avoid introducing yet more parameters and (b) as a way to introduce additional regularization in a way that supports the standard Matchbox~\cite{matchbox} CF model where prediction at run-time is made for a (user,item) pair, not for triples of (user,item,tag) as assumed in the tensor models.
\end{enumerate}

\subsection{Collaborative Filtering Algorithms}

{\bf $k$-Nearest Neighbor}
\\

The $k$-nearest neighbor algorithm is a method of pattern recognition that is based on the $k$ closest training data in the feature space. There are two main variants of nearest neighbors for collaborative recommendation, user-based and item-based. Given a user $u$ and an item $i$, let $N(u:i)$ be the set of nearest neighbors of $u$ that have also given a rating for i, $N(i:u)$ be the set of nearest neighbors of $i$ that have also been rated by $u$, $s_{uu'}$ the similarity rating between users $u$ and $u'$, and $s_{ii'}$ be the similarity rating for items $i$ and $i'$. The predicted rating the user $u$ gives item $i$ in the user-based approach is then calculated as

\[
r_{ui} = \frac    {\sum_{v\in N(u;i)} {s_{uv}r_{uv}} } {\sum_{v\in N(u;i)}{s_{uv}}}
\]

The item-based approach is calculated as 

\[
r_{ui} = \frac    {\sum_{j\in N(i;u)} {s_{ij}r_{uj}} } {\sum_{j\in N(i;u)}{s_{ij}}}
\]

The question of which approach to use depends on the dataset. When the number of items is far fewer than the number of users, it has been found that the item-based approach usually provides better predictions as well as being more efficient in computations. 
\\

\begin{comment}
This applies to the MovieLens 1 Million dataset as well. For the MovieLens 100,000 dataset, the number of items is larger than the number of users, and the user-based approach has been found to perform better.
\end{comment}


{\bf Support Vector Machines}
\\

Support Vector Machines are a class of supervised learning classification algorithms that uses a hyperplane separating approach. During training, SVM builds a model by constructing a set of hyperplanes that separates one class of data from another class with the maximum margin possible. Data are classified by finding out on which side of a hyperplane they fall under.

For the experiments, SVM uses a fixed-length feature vector
$\f \in \mathbb{R}^F$ derived from any $(\x,\y) \in D$, denoted
as $\f_{\x,\y}$.  $\f_{\x,\y}$ may include features
that are non-zero only for specific items and/or users, e.g., a $\{0,1\}$ 
indicator feature that user $\x$
and user $\z$ have both liked item $\y$.  

\subsection{Matrix Factorization Models}

As done in standard CF methods, we assume that
a matrix $U$ allows us to project users $\x$ (and $\z$)
into a latent space of dimensionality $K$; likewise we assume that
a matrix $V$ allows us to project items $\y$ into a latent
space also of dimensionality $K$.  Formally we define $U$ and $V$
as follows:

\begin{equation*}
U = 
\begin{bmatrix}
  U_{1,1} & \hdots  & U_{1,I} \\
  \vdots  & U_{k,i} & \vdots  \\
  U_{K,1} & \hdots  & U_{K,I} \\
\end{bmatrix}
\qquad \; \; \;
V = 
\begin{bmatrix}
  V_{1,1} & \hdots  & V_{1,J} \\
  \vdots  & V_{k,j} & \vdots  \\
  V_{K,1} & \hdots  & V_{K,J} \\
\end{bmatrix}
\end{equation*}
Now we can respectively represent the latent projections of user and
item as $(U \x)_{1 \ldots K}$ and $(V \y)_{1 \ldots K}$ and
hence use $\la U \x, V \y \ra = \x^T U^T V \y$ as a latent bilinear regressor. 

%The objective component for this model that we seek to minimize is:


%\begin{align}
%\sum_{(\x,\y) \in D} \frac{1}{2} (R_{\x,\y} - [\sigma] \x^T U^T V y)^2
%\end{align}

\subsection{Social Collaborative Filtering}
%These are MF methods that use the social network and do recommendation,
%note that the GLFM and Bidirectional similarity papers do not meet these
%requirements

There are essentially two general classes of MF methods applied to SCF that we discuss
below.  The first class can be termed as \emph{social regularization}
approaches in that they somehow constrain the latent projection
represented by $U$.  

There are two social regularization methods that directly constrain $U$ for user $i$
and $k$ based on evidence $S_{i,k}$ of interaction between $i$ and $k$.  We call
these methods:

% Note that these previous methods do not use user and item features
\begin{itemize}
\item {\bf Social regularization~\cite{lla,socinf}}:
\begin{align}
\sum_{i} & \sum_{k \in \mathit{friends}(i)} \frac{1}{2} (S_{i,k} - \la U_i, U_k \ra)^2 \nonumber 
\end{align}

\item {\bf Social spectral regularization~\cite{sr,rrmf}}:
\begin{align}
\sum_{i} & \sum_{k \in \mathit{friends}(i)} \frac{1}{2} S^+_{i,k} \| U_i - U_k \|_2^2 \nonumber
\end{align}
\end{itemize}

The {\it SoRec} system~\cite{sorec} proposes a slight twist on social
spectral regularization in that it learns a third ($N \times N$)
\emph{interactions matrix} $Z$ used $U_i^T Z_k$ to predict user-user
interaction preferences in the same way that standard CF uses $V$ in
$U_i^T V_j$ to predict user-item ratings.  {\it SoRec} also uses a
sigmoidal transform on the predictions.

\begin{itemize}
\item {\bf SoRec regularization~\cite{sorec}}:
\begin{align}
\sum_{i} & \sum_{k \in \mathit{friends}(i)} \frac{1}{2} (S_{i,k} - \sigma(\la U_i, Z_k \ra))^2 \nonumber
\end{align}
\end{itemize}

The second class of SCF MF approaches represented by the single
examplar of the {\it Social Trust Ensemble} can be termed as a
\emph{weighted average} approach since this approach simply composes a
prediction for item $j$ from a weighted average of a user $i$'s
predictio
ns \emph{as well as} their friends ($k$) predictions (as
evidenced by the additional $\sum_k$ in the objective below).

\begin{itemize}
\item {\bf Social Trust Ensemble~\cite{ste} (Non-spectral)}:
\begin{align}
\sum_{(i,j) \in D} \frac{1}{2} (R_{i,j} - \sigma (U_i^T V_j + \sum_k U_i^T V_k))^2 \nonumber
\end{align}
\end{itemize}

\subsection{Tensor Factorization Methods}

% Note: not social CF... just another MF variant for recommendation

\emph{Tensor factorization} (TF) methods can be used to learn latent
models of interaction of 2 dimensions and higher.  A dimension 2 TF
method is simply standard MF.  An example of a dimension 3 TF method
is given by~\cite{tf} where recommendation of user-specific tags for
an item are modeled with tags, user, and items each in one dimension.
To date, TF methods have not been used for social recommendation,
however, we draw on the idea of addition dimensions of latent learning
in our copreference regularization method.


